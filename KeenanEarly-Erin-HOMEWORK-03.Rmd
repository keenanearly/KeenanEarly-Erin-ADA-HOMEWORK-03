---
title: "KeenanEarly-Erin-HOMEWORK-03"
author: "Erin Keenan Early"
date: "March 22, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1

Write a simple R function you call Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines.

Your function should take the following arguments: p1 and n1 (no default) representing the estimated proportion and sample size (e.g., based on your sample data); p2 and n2 (both defaulting to NULL) that contain a second sample’s proportion and sample size data in the event of a two-sample test; p0 (no default) as the expected value for the population proportion; and alternative (default “two.sided”) and conf.level (default 0.95), to be used in the same way as in the function t.test().

When conducting a two-sample test, it should be p1 that is tested as being smaller or larger than p2 when alternative=“less” or alternative=“greater”, respectively, the same as in the use of x and y in the function t.test().

The function should perform a one-sample Z-test using p1, n1, and p0 if either p2 or n2 (or both) is NULL.

The function should contain a check for the rules of thumb we have talked about (n×π>5 and n×(1−π)>5) to ensure the validity of assuming the normal distribution in both the one- and two-sample settings. If this is violated, the function should still complete, but it should also print an appropriate warning message.

The function should return a list containing the following elements: Z (the test statistic), P (the appropriate p value), and CI (the two-sided CI with respect to “conf.level” around p1 in the case of a one-sample test and around p2-p1 in the case of a two-sample test). For all test alternatives (“two.sided”, “greater”, “less”), calculate symmetric CIs based on quantiles of the normal distribution rather than worrying about calculating single-limit confidence bounds.


```{r}
Z.prop.test<- function(p1, n1, p2=NULL, n2=NULL, p0, alternative="two.sided", conf.level=0.95){
  
  if(!((n1*p0)>5 & (n1*(1-p0)>5))) {print("WARNING: Your data is questionable")
  }

  if(is.null(p2) || is.null(n2)){
  z<- ((p1-p0)/sqrt((p0*(1-p0))/n1)) #defines my output z for one-sided
  }
  else{
    if(!((n2*p0)>5 & (n2*(1-p0)>5))) {print("WARNING: Your other data is questionable")
    }
    pstar<-(p1*n1+p2*n2)/(n1+n2)
    z<-((p1-p2-p0)/sqrt(pstar*(1-pstar)*((1/n1)+(1/n2)))) #defnes my output z for two-sided 
  }
  
  if(alternative=="greater"){
     p<- pnorm(z, lower.tail = TRUE) #defines my output p for lesser
  }
  else if(alternative == "less"){
    p<- 1-pnorm(z, lower.tail=TRUE) #defines my output p for greater
  }
  else if(alternative=="two.sided"){
    p.upper<- 1-pnorm(abs(z), lower.tail = TRUE)
    p.lower<- pnorm(abs(z), lower.tail = FALSE)
    p<- p.upper+p.lower
  }
  alpha<- 1-conf.level  
  CIlower<- p1-qnorm(1-alpha/2)*sqrt(p1*(1-p1)/n1)
  CIupper<- p1+qnorm(1-alpha/2)*sqrt(p1*(1-p1)/n1)
  CI<- c(CIlower, CIupper)
  
  return(list(z, p, CI)) #defines my outputs
}

Z.prop.test(p1=.1, n1=50, p0=0.5) #Test of two-sided function
Z.prop.test(p1=0.1,n1=50, p0=0.5, alternative="less") #Test of alternative one-sided function
Z.prop.test(p1=0.1,n1=50, p0=0.5, alternative="greater") #Test of alternative one-sided function
```


```{r}
Z.prop.test(p1=0.4, n1=50, p2=0.3, n2= 70, p0=0) #Test of two-sided function two sample
Z.prop.test(p1=0.4, n1=50, p2=0.3, n2= 70, p0=0, alternative="less") #Test of alternative one-sided function two sample
Z.prop.test(p1=0.4, n1=50, p2=0.3, n2= 70, p0=0, alternative="greater") #Test of alternative one-sided function two sample
```
**Tony**: I do not understand how I would NOT get my warning for a two-sample test when the "rule of thumb" requires that p0(pi) not be zero... Can you explain this to me? Thank you!

```{r}
#Test validity of data
Z.prop.test(p1=.1, n1=3, p0=0.5) #Test of two-sided function
```


```{r}
Z.prop.test(p1=0.4, n1=50, p2=0.3, n2= 70, p0=0) #Test of two-sided function two sample
```


## Problem 2


The comparative primate dataset we have used from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size).

```{r}
library(readr)
f<- "https://raw.githubusercontent.com/difiore/ADA-2019/master/KamilarAndCooperData.csv"
d<- read_csv(f, col_names = TRUE)
```

```{r} 
head(d)
```

Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).

```{r}
library(ggplot2)
library(tidyverse)
library(readr)
brainsize<- d$Brain_Size_Species_Mean
longevity<- d$MaxLongevity_m 
loglong<- log(longevity)
logbrain<-log(brainsize)
reg1<- lm(data = d, longevity~brainsize) #regression model for logevity as a function of brainsize
reg2<- lm(data=d, loglong~logbrain) #regression model for (log)longevity as a function of (log)brainsize
reg1
reg2
```

Non-log transformed data

```{r}
plot1<- ggplot(data=d, aes(x= brainsize, y = longevity))
plot1<- plot1+geom_point()
plot1<- plot1+ggtitle("Linear Regression of Longevity as a Function of Brainsize")+theme(plot.title = element_text(hjust = 0.5))
plot1<- plot1+xlab("Brainsize of Species (grams)")
plot1<- plot1+ylab("Longevity of Species (months)")
plot1<- plot1+geom_smooth(method = "lm", formula = y~x, colour="red")
plot1<- plot1+geom_text(aes(x=brainsize, y=longevity, label=" "))+annotate("text", label="intercept=248.952, slope=1.218", x=100, y=0, size=5, colour="blue")
plot1
```

Regression of log-transformed data

```{r}
plot2<- ggplot(data=d, aes(x= logbrain, y = loglong))
plot2<- plot2+geom_point()
plot2<- plot2+ggtitle("Linear Regression of (log)Longevity as a Function of (log)Brainsize")+theme(plot.title = element_text(hjust = 0.5))
plot2<- plot2+xlab("(log)Brainsize of Species (grams)")
plot2<- plot2+ylab("(log)Longevity of Species (months)")
plot2<- plot2+geom_smooth(method = "lm", formula = y~x, colour="blue")
plot2<- plot2+geom_text(aes(x=logbrain, y=loglong, label=" "))+annotate("text", label="intercept=4.8790, slope=0.2341", x=2, y=4.5, size=5, colour="hot pink")
plot2
```


Identify and interpret the point estimate of the slope (β1...:

β1 indicates the expected number of months a species will live based on the brainsize in grams of that species.β1 when calculated from longevity as a function of brainsize is 1.218, but when log transformed such that it is calculated from (log)longevity as a function of (log)brainsize β1 is 0.2341. 


...as well as the outcome of the test associated with the hypotheses H0: β1=0; HA: β1≠0: 










Also, find a 90% CI for the slope (β1) parameter.

```{r}
t1<-coef(summary(reg1))
t1<-data.frame(t1)
t2<-coef(summary(reg2))
t2<-data.frame(t2)
```

```{r}
alpha<- 0.1
t1CI<- confint(reg1, level=1-alpha)
t1CI
```

Using your model, add lines for the 90% confidence and prediction interval bands on the plot, and add a legend to differentiate between the lines.
```{r}
lowerCI<-t1$Estimate-qt(1-alpha/2, df=126)*t1$Std..Error
upperCI<-t1$Estimate+qt(1-alpha/2, df=126)*t1$Std..Error
t1Conf<-cbind(lowerCI,upperCI)
t1Conf
```

```{r}
library(ggplot2)
t1Conf<- data.frame(t1Conf)
plot1<- ggplot(data=d, aes(x= brainsize, y = longevity))
plot1<- plot1+geom_point()
plot1<- plot1+ggtitle("Linear Regression of Longevity as a Function of Brainsize")+theme(plot.title = element_text(hjust = 0.5))
plot1<- plot1+xlab("Brainsize of Species (grams)")
plot1<- plot1+ylab("Longevity of Species (months)")
plot1<- plot1+geom_smooth(method = "lm", formula = y~x, colour="red")
plot1<- plot1+geom_text(aes(x=brainsize, y=longevity, label=" "))+annotate("text", label="intercept=248.952, slope=1.218", x=100, y=0, size=5, colour="blue")
plot1<- plot1+geom_point(alpha=0.5)
plot1<- plot1+geom_abline(slope=1.035571, intercept=230.540738, col="purple")
plot1<- plot1+geom_abline(slope=1.40041, intercept = 267.36379, col="blue")
plot1<- plot1+annotate("text", label="LowerCI(purple) int=230.540738 slope=1.035571", x=150, y=850)
plot1<- plot1+annotate("text", label="UpperCI(blue) int=267.36379 slope=1.40041", x=150, y=800)
plot1
```

```{r}
summary(reg2)
```


```{r}
alpha<- 0.1
t2CI<- confint(reg2, level=1-alpha)
t2CI
```

```{r}
lowerCI<-t2$Estimate-qt(1-alpha/2, df=126)*t2$Std..Error
upperCI<-t2$Estimate+qt(1-alpha/2, df=126)*t2$Std..Error
t2Conf<-cbind(lowerCI,upperCI)
t2Conf
```

```{r}
plot2<- ggplot(data=d, aes(x= logbrain, y = loglong))
plot2<- plot2+geom_point()
plot2<- plot2+ggtitle("Linear Regression of (log)Longevity as a Function of (log)Brainsize")+theme(plot.title = element_text(hjust = 0.5))
plot2<- plot2+xlab("(log)Brainsize of Species (grams)")
plot2<- plot2+ylab("(log)Longevity of Species (months)")
plot2<- plot2+geom_smooth(method = "lm", formula = y~x, colour="blue")
plot2<- plot2+geom_text(aes(x=logbrain, y=loglong, label=" "))+annotate("text", label="intercept=4.8790, slope=0.2341", x=2, y=4.5, size=5, colour="hot pink")
plot2<- plot2+geom_point(alpha=0.5)
plot2<- plot2+geom_abline(slope=0.2046396, intercept=4.7644934, col="red")
plot2<- plot2+geom_abline(slope=0.2636595, intercept = 4.9934084, col="orange")
plot2<- plot2+annotate("text", label="LowerCI(red) int=230.540738 slope=1.035571", x=2, y=6.5)
plot2<- plot2+annotate("text", label="UpperCI(orange) int=267.36379 slope=1.40041", x=2, y=6.35)
plot2
```





Produce a point estimate and associated 90% prediction interval for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?

```{r}
beta01<- t1$Estimate[1]
beta11<- t1$Estimate[2]
l_hat<- beta11*800+beta01
l_hat
```

```{r}
beta02<- t2$Estimate[1]
beta12<- t2$Estimate[2]
logl_hat<- beta12*log(800)+beta02
logl_hat
```

Looking at your two models, which do you think is better? Why?

The log transformed model is the better model as it provides a better visualization of the data. 






